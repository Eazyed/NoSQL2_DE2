// Le nom de l’agent est dans ce cas « target_agent »
// Définition du nom de la source
target_agent.sources = kafkaSource
// Définition du nom du channel
target_agent.channels = memoryChannel
// Définition du nom du sink
target_agent.sinks = hiveSink

// Définition du type de source, ici une source Kafka
target_agent.sources.kafkaSource.type = org.apache.flume.source.kafka.KafkaSource
// Connexion au cluster Kafka
target_agent.sources.kafkaSource.bootstrap.servers = // A remplir
//Nom du topic auquel souscrire dans Kafka 
target_agent.sources.kafkaSource.topic = // A remplir
// Nom du channel
target_agent.sources.kafkaSource.channels = memoryChannel
// Id du groupe d’agent flume
target_agent.sources.kafkaSource.groupId = // A déterminer

// Définition du type de channel
target_agent.channels.memoryChannel.type = memory
// Réglage de capacité
target_agent.channels.memoryChannel.capacity = 100

// Configuration du sink de sortie vers Hive
// Type de sink
target_agent.sinks.hiveSink.type = hive
// Channel d’entrée
target_agent.sinks. hiveSink.channel = memoryChannel
// Stockage des méta-données
target_agent.sinks. hiveSink.hive.metastore = // A determiner
// Nom de la database Hive de sortie
target_agent.sinks. hiveSink.database = LinkyDB
// Nom de la table Hive de sortie
target_agent.sinks. hiveSink.table = eventData
//Partitionnement des données dans la table par date
target_agent.sinks. hiveSink.hive.partition =%y-%m-%d
// On utilise le TimeStamp UTC
target_agent.sinks. hiveSink.useLocalTimeStamp = false


